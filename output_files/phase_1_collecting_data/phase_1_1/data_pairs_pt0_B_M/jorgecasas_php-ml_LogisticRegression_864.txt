    /**
     * Returns the appropriate callback function for the selected cost function
     *
     * @throws \Exception
     */

    protected function getCostFunction(): Closure
    {
        $penalty = 0;
        if ($this->penalty === 'L2') {
            $penalty = $this->lambda;
        }

        switch ($this->costFunction) {
            case 'log':
                /*
                 * Negative of Log-likelihood cost function to be minimized:
                 *		J(x) = ∑( - y . log(h(x)) - (1 - y) . log(1 - h(x)))
                 *
                 * If regularization term is given, then it will be added to the cost:
                 *		for L2 : J(x) = J(x) +  λ/m . w
                 *
                 * The gradient of the cost function to be used with gradient descent:
                 *		∇J(x) = -(y - h(x)) = (h(x) - y)
                 */
                return function ($weights, $sample, $y) use ($penalty): array {
                    $this->weights = $weights;
                    $hX = $this->output($sample);

                    // In cases where $hX = 1 or $hX = 0, the log-likelihood
                    // value will give a NaN, so we fix these values
                    if ($hX == 1) {
                        $hX = 1 - 1e-10;
                    }

                    if ($hX == 0) {
                        $hX = 1e-10;
                    }

                    $y = $y < 0 ? 0 : 1;

                    $error = -$y * log($hX) - (1 - $y) * log(1 - $hX);
                    $gradient = $hX - $y;

                    return [$error, $gradient, $penalty];
                };
            case 'sse':
                /*
                 * Sum of squared errors or least squared errors cost function:
                 *		J(x) = ∑ (y - h(x))^2
                 *
                 * If regularization term is given, then it will be added to the cost:
                 *		for L2 : J(x) = J(x) +  λ/m . w
                 *
                 * The gradient of the cost function:
                 *		∇J(x) = -(h(x) - y) . h(x) . (1 - h(x))
                 */
                return function ($weights, $sample, $y) use ($penalty): array {
                    $this->weights = $weights;
                    $hX = $this->output($sample);

                    $y = $y < 0 ? 0 : 1;

                    $error = (($y - $hX) ** 2);
                    $gradient = -($y - $hX) * $hX * (1 - $hX);

                    return [$error, $gradient, $penalty];
                };
            default:
                // Not reached
                throw new Exception(sprintf('Logistic regression has invalid cost function: %s.', $this->costFunction));
        }
    }
